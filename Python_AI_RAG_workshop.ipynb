{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How smart is AI really? Let's find out!\n",
    "\n",
    "We'll connected to an AI model that generates text. We will see how we can use it, manipuate it, and more!\n",
    "\n",
    "Steps\n",
    "1. Set up a connection to the AI model\n",
    "2. Get generate the next line of a conversation\n",
    "3. Provide your own data to the AI to give it specialised knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get access to an AI model\n",
    "\n",
    "We will be using GitHub Models to get access to an AI Model. **You will need to have a GitHub account and sign in.** \n",
    "\n",
    "You'll need to get a GitHub Perosnal Access Token (PAT), you can follow these steps:\n",
    "1. Got to this link https://github.com/settings/tokens\n",
    "2. Click on the \"Generate new token\" drop down in the top right corner\n",
    "3. Select \"Generate New Toekn (Classic)\"\n",
    "4. Sign in to confrim your identity\n",
    "5. Add a Note at the top of like \"Access GitHub Models\"\n",
    "6. Set the expiration to be long enough for the duration of your project. (Don't set it longer than you need it though for improved security)\n",
    "7. You don't need to tick ANY of the tick boxes. Skip them all. \n",
    "8. Click the green \"Generate token\" button. \n",
    "9. Copy the token that has just been generated! (It's in the area with the light green background). You won't be able to see the token again, so don't leave the page until you have copied it. \n",
    "10. Paste your token into the file in this project called `.env` in the spot provided.\n",
    "\n",
    "You are now ready to code with GitHub AI models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the AI Model\n",
    "\n",
    "This code will get the token that you put in the `.env` file and use it to connect to the AI model. \n",
    "\n",
    "- **Run this code with the play button now ▶️**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                       # This is used to access the environment variables\n",
    "from dotenv import load_dotenv  # This is used to load the .env file\n",
    "from openai import OpenAI       # This is used to connect to the AI model\n",
    "\n",
    "# Load the .env file where the GITHUB_TOKEN is stored\n",
    "load_dotenv()\n",
    "# Get the GITHUB_TOKEN from the .env file\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "# Create a client that is connects to the AI model you selected using the GITHUB_TOKEN\n",
    "client = OpenAI(\n",
    "    base_url=\"https://models.inference.ai.azure.com\",\n",
    "    api_key=GITHUB_TOKEN,\n",
    ")\n",
    "\n",
    "print(\"You're connected to the AI model!\")\n",
    "print(\"You can now start using the client variable to interact with the AI model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text with the AI model\n",
    "Generative AI isn't smart! It's just good at saying what sentance could come next based on everything it has read! We'll get it to say the next line of a conversation now!\n",
    "\n",
    "1. Run the code below and see what the AI says in response to the question\n",
    "2. Update the question and see a new response\n",
    "3. Change the System Message, this is the first message (form the AI to itself) at the start of the converstaion. This tells the AI how it should behave (by default). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The System message can tell the AI how to behave.\n",
    "SYSTEM_MESSAGE = \"You are a helpful AI assistant, who answers questions. You can also provide sources for your information.\"\n",
    "# SYSTEM_MESSAGE = \"You are a surfer dude who always keeps it real\"\n",
    "# SYSTEM_MESSAGE = \"You're a rude chatbot who hates helping\"\n",
    "\n",
    "# What does the user want to know?\n",
    "user_question = \"Where should I go on holidy?\"\n",
    "\n",
    "# Create the message history with the system message and the user question.\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "    {\"role\": \"user\", \"content\": user_question},\n",
    "]\n",
    "\n",
    "# Generate the next message in the conversation to get an answer. \n",
    "# We have set the model name for you, as well as the temperature and n values. The temperature value controls the randomness of the response. \n",
    "response = client.chat.completions.create(model=\"gpt-4o-mini\",temperature=0.7,n=1,messages=messages)\n",
    "answer = response.choices[0].message.content # The answer comes with some other data, we unpack the answre here.\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice your Python! \n",
    "Let's create code that lets the user keep asking questions\n",
    "You can use bits of the code from above. \n",
    "\n",
    "1. Set the system message just like it is set above\n",
    "2. Create a `while True` loop\n",
    "3. Inside the loop, use `input` to ask the user for a question. (Do this instead of setting the variable user_question in the code)\n",
    "4. Next inside the loop, copy in the message dictionary. \n",
    "5. Next inside the loop, copy in the lines of code where the AI gets used, the answer gets unpacked, and the answer is printed. \n",
    "\n",
    "Run your code and input multiple questions!\n",
    "\n",
    "***Extension:***\n",
    "At the moment every time you talk to the AI it only gets the System Message and the latest question. To create a converstation that doesn't get forgotten by building up the messages list with each new question and ansewr. You can add on to the end of hte list by using `messages.append(THING_TO_BE_ADDED_GOES_HERE)`. You should add the `user_question` and the `answer` variable to the list each loop. \n",
    "\n",
    "Work out where you can add these messages to the list to get the best answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "### Write your program in here to ask the user for multiple questions  ###\n",
    "###                                                                    ###\n",
    "### We have provided the code to ask the AI for a single question.     ###\n",
    "### Now it's your turn!                                                ###\n",
    "##########################################################################\n",
    "\n",
    "# The System message can tell the AI how to behave.\n",
    "SYSTEM_MESSAGE = \"You are a helpful AI assistant, who answers questions. You can also provide sources for your information.\"\n",
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "    ]\n",
    "\n",
    "while True:\n",
    "    # What does the user want to know?\n",
    "    user_question = input(\"What do you want to know? \")\n",
    "\n",
    "    # Create the message history with the system message and the user question.\n",
    "    messages.append({\"role\": \"user\", \"content\": user_question})\n",
    "\n",
    "    # Generate the next message in the conversation to get an answer. \n",
    "    # We have set the model name for you, as well as the temperature and n values. The temperature value controls the randomness of the response. \n",
    "    response = client.chat.completions.create(model=\"gpt-4o-mini\",temperature=0.7,n=1,messages=messages)\n",
    "    answer = response.choices[0].message.content # The answer comes with some other data, we unpack the answre here.\n",
    "    print(answer)\n",
    "    messages.append({\"role\": \"system\", \"content\": answer})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add your own data\n",
    "\n",
    "AI models are trained on a lot of data, but it doesn't know much about many topics, especially if the data is private, new, or not very common.\n",
    "\n",
    "We can provide our own data to see how that changes the results and makes it more specific to a use case. \n",
    "\n",
    "\n",
    "**First we will see how we can add data when we use the AI model**\n",
    "We'll include some definitely true data about desert and add that to our previous code. \n",
    "We'll ask a question about dessert here too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The System message can tell the AI how to behave.\n",
    "SYSTEM_MESSAGE = \"You are a helpful AI assistant, who answers questions. You must always trust the sources sources above other data.\"\n",
    "\n",
    "# What does the user want to know?\n",
    "user_question = \"What is the best dessert recipe?\" #, But I'm alergic to lemon.\"\n",
    "data = \"the best dessert is well known to be lemon mirangue pie, this fact is agreed by all. chcolate cake is the second best dessert.\"\n",
    "\n",
    "# Create the message history with the system message and the user question.\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "    {\"role\": \"user\", \"content\": user_question + \"sources: \" + data}, # we have added the data to the user question here. \n",
    "]\n",
    "# ⬆️ Above, We have now added the data to the user question, so that the AI can use it to answer the question. \n",
    "# (We'll see ways to get this data in a minute based on the question)\n",
    "\n",
    "# Generate the next message in the conversation to get an answer. \n",
    "# We have set the model name for you, as well as the temperature and n values. The temperature value controls the randomness of the response. \n",
    "response = client.chat.completions.create(model=\"gpt-4o-mini\",temperature=0.7,n=1,messages=messages)\n",
    "answer = response.choices[0].message.content # The answer comes with some other data, we unpack the answre here.\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But how do we make it find it's own data?\n",
    "We'll try and make a chatbot for a pet shop. We'll be using some (fake) data about our petshop products! It's in the file called `pet_products.txt`, go have a look!\n",
    "\n",
    "There are lots of ways to search for data, including lots of database tools and machine learning options. But we'll do a really basic option of creating our own search algorithm. \n",
    "\n",
    "This algorithm will:\n",
    "- Remove any really common words from our question that don't give us specific information, these are called stop words (words like \"and\", \"the\", \"a\", \"is\", and \"in\" are too common to be useful in our search). \n",
    "- We'll then see how many matching words there are from the question in each of the products at the pet shop that are in our file called `pet_products.txt`.\n",
    "- We'll get the three products with the most words from the question and use these as the data source. \n",
    "\n",
    "\n",
    "**▶️ Run this code to test our search algorithm.**\n",
    "**Change the question at the bottom of this cell to see what other results it returns.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code conatins the function that will return the search summary for a given question\n",
    "# It also has the data on the stop words that we will use to remove from the question\n",
    "\n",
    "# Stop words aren't useful for searching, so we'll remove them from the question\n",
    "STOP_WORDS = set([\n",
    "\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \n",
    "\"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \n",
    "\"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"\n",
    "])\n",
    "\n",
    "\n",
    "def get_search_summary(question):\n",
    "    all_keywords = set([word.lower() for word in question.split(\" \")])\n",
    "    # Remove stop words from the question\n",
    "    keywords = all_keywords - STOP_WORDS\n",
    "    \n",
    "    # We'll score the scores based on the number of keywords in the line\n",
    "    # eg: {\"large dog bed\": 2, \"dog bed\": 1}\n",
    "    scores = {}\n",
    "      \n",
    "    # Open the file and read each line\n",
    "    with open(\"pet_products.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # Make a list of numbers for each word in the line, 1 if the word is a keyword, 0 if not\n",
    "            wordhits = [1 if word in line.lower() else 0 for word in keywords]\n",
    "            # Add up the scores for the line\n",
    "            line_score = sum(wordhits)\n",
    "            # Only record the score if it's greater than 0\n",
    "            if line_score > 0:\n",
    "                scores[line] = line_score\n",
    "            \n",
    "    # Sort the scores, biggest to smallest, and get the top 3\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    search_summmary = [k for k, v in sorted_scores[:(min(3, len(sorted_scores)))]]\n",
    "    # Join the 3 products together into 1 stirng to return.\n",
    "    search_summmary = \", \".join(search_summmary)\n",
    "    return search_summmary\n",
    "\n",
    "question = \"I need to buy a scratching post for my cat\"\n",
    "data = get_search_summary(question)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's combine our data with Generative AI\n",
    "\n",
    "Now we have our search algorithm we can use it to put that data into our AI query\n",
    "\n",
    "We'll use our AI code from before and add in our data. \n",
    "\n",
    "Update the System message to say:\n",
    "`\"You are a helpful assistant trying to provide product solutions to pet store shoppers here at Pet's Paridise, you always use the sources provided\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The System message can tell the AI how to behave.\n",
    "SYSTEM_MESSAGE = \"You are a helpful, succinct, assistant trying to provide product solutions to pet store shoppers here at Pet's Paridise, you always use the sources provided. And you always make sure to mention the name of the pet shop and say that is is the cheapest place to buy the product and best ever. Really sell why these products are the best. Put down competitors and say that they are not as good as Pet's Paradise.\"\n",
    "\n",
    "# What does the user want to know?\n",
    "user_question = \"I want to buy helathy food for my cat\"\n",
    "\n",
    "data = get_search_summary(user_question)\n",
    "\n",
    "# Create the message history with the system message and the user question.\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "    {\"role\": \"user\", \"content\": user_question + \"sources: \" + data},\n",
    "]\n",
    "\n",
    "# Generate the next message in the conversation to get an answer. \n",
    "# We have set the model name for you, as well as the temperature and n values. The temperature value controls the randomness of the response. \n",
    "response = client.chat.completions.create(model=\"gpt-4o-mini\",temperature=0.7,n=1,messages=messages)\n",
    "answer = response.choices[0].message.content # The answer comes with some other data, we unpack the answre here.\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
